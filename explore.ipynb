{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General setup\n",
    "\n",
    "Import some general stuff, set up the sagemaker environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use typing to help our logic\n",
    "import typing\n",
    "from typing import List, Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto3.session import Session as BotoSession\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "\n",
    "boto_session = BotoSession(os.environ.get(\"AWS_ACCESS_KEY_ID\"), os.environ.get(\"AWS_SECRET_ACCESS_KEY\"))\n",
    "region = boto_session.region_name\n",
    "session = Session(boto_session=boto_session)\n",
    "role = 'rolesAmazonSageMaker-ExecutionRole-20190703T193673'\n",
    "bucket = 'sagemaker-eu-west-1-729071960169'\n",
    "bucket_folder = \"numerai\"\n",
    "prefix = f\"{bucket_folder}/explore\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Second step is formatting the data, splitting it in several batches and splitting it for training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"data/numerai_training_data.csv\", index_col=\"id\")\n",
    "tournament = pd.read_csv(\"data/numerai_tournament_data.csv\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(df: DataFrame, number_of_batches: int) -> DataFrame:\n",
    "    \"\"\"\n",
    "    This method splits the dataframe into different batches.\n",
    "    Arguments:\n",
    "        df: The dataframe to split.\n",
    "        number_of_batches: The number of batches\n",
    "    \"\"\"\n",
    "    # Suffle the df first so it's actually random\n",
    "    intermediate = df.sample(frac=1)\n",
    "    number_of_rows = df.shape[0]\n",
    "    list_of_dataframes = []\n",
    "    \n",
    "    for i in range(number_of_batches):\n",
    "        start_index = (i * number_of_rows) // number_of_batches\n",
    "        end_index = ((i + 1) * number_of_rows) // number_of_batches\n",
    "        list_of_dataframes.append(df.iloc[start_index:end_index])\n",
    "        \n",
    "    return list_of_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_batches = 10\n",
    "list_of_dataframes = create_batches(training_data, number_of_batches)\n",
    "\n",
    "# To be sure we have all items\n",
    "assert sum([item.shape[0] for item in list_of_dataframes]) == training_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def split_train_validation_test(df: DataFrame, validation_frac: float, test_frac: float) -> Tuple[DataFrame, Optional[DataFrame], Optional[DataFrame]]:\n",
    "    \"\"\"\n",
    "    Splits the dataframe in a train, validation and test dataframe, based on the parameters.\n",
    "    validation_frac and test_frac must be larger or equal to 0, the sum must be between 0 and 1, not inclusive\n",
    "    \n",
    "    Arguments:\n",
    "        df: The dataframe to split\n",
    "        validation_frac: The fraction for the validation set\n",
    "        test_frac: The fraction for the test test\n",
    "        \n",
    "    Returns:\n",
    "        A tuple of 3 dataframes: Train, Validation, Test.\n",
    "        If validation_frac or test_frac was 0, that dataframe will be None\n",
    "    \"\"\"\n",
    "    assert 0 < validation_frac + test_frac < 1\n",
    "    assert validation_frac >= 0 and test_frac >= 0\n",
    "    test_val_frac = validation_frac + test_frac\n",
    "    train, validation = train_test_split(df, test_size=test_val_frac)\n",
    "    validation_over_test_frac = validation_frac / test_val_frac if test_frac != 0 else 1\n",
    "    if validation_over_test_frac == 1:\n",
    "        test = None\n",
    "    elif validation_over_test_frac == 0:\n",
    "        test = validation\n",
    "        validation = None\n",
    "    else:\n",
    "        validation, test = train_test_split(validation, train_size=validation_over_test_frac)\n",
    "    \n",
    "    return (train, validation, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the datasets into train, validation and test\n",
    "list_of_split_dataframes = []\n",
    "for i, batch in enumerate(list_of_dataframes):\n",
    "    (train, validation, test) = split_train_validation_test(batch, 0.2, 0.2)\n",
    "    list_of_split_dataframes.append({\n",
    "        \"train\": train,\n",
    "        \"validation\": validation,\n",
    "        \"test\": test,\n",
    "    })\n",
    "list_of_dataframes = list_of_split_dataframes\n",
    "list_of_split_dataframes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10037, 313)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataframe into features and output\n",
    "list_of_split_dataframes = []\n",
    "for batch in list_of_dataframes:\n",
    "    temp_dict = {}\n",
    "    for data_type, dataframe in batch.items():\n",
    "        if dataframe is not None:\n",
    "            y_data = dataframe.iloc[:, -1]\n",
    "            temp_dict[f\"Y_{data_type}\"] = y_data\n",
    "            x_data = dataframe.iloc[:, 2: -1]\n",
    "            temp_dict[f\"X_{data_type}\"] = x_data\n",
    "    list_of_split_dataframes.append(temp_dict)\n",
    "list_of_dataframes = list_of_split_dataframes\n",
    "list_of_split_dataframes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload everything to S3\n",
    "list_of_s3_names = []\n",
    "for i, batch in enumerate(list_of_dataframes):\n",
    "    temp_dict = {}\n",
    "    for data_type, dataframe in batch.items():\n",
    "        if dataframe is not None:\n",
    "            path_name = f\"batch_{i}_{data_type}.csv\"\n",
    "            path_name = os.path.join(\"data\", path_name)\n",
    "            dataframe.to_csv(path_name, index=False, header=False)\n",
    "            temp_dict[data_type] = session.upload_data(path_name, bucket=bucket, key_prefix=prefix)\n",
    "        else:\n",
    "            temp_dict[data_type] = None\n",
    "    list_of_s3_names.append(temp_dict)\n",
    "list_of_dataframes = list_of_s3_names\n",
    "list_of_s3_names = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Y_train': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_0_Y_train.csv',\n",
       "  'X_train': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_0_X_train.csv',\n",
       "  'Y_validation': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_0_Y_validation.csv',\n",
       "  'X_validation': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_0_X_validation.csv',\n",
       "  'Y_test': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_0_Y_test.csv',\n",
       "  'X_test': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_0_X_test.csv'},\n",
       " {'Y_train': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_1_Y_train.csv',\n",
       "  'X_train': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_1_X_train.csv',\n",
       "  'Y_validation': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_1_Y_validation.csv',\n",
       "  'X_validation': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_1_X_validation.csv',\n",
       "  'Y_test': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_1_Y_test.csv',\n",
       "  'X_test': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_1_X_test.csv'},\n",
       " {'Y_train': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_2_Y_train.csv',\n",
       "  'X_train': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_2_X_train.csv',\n",
       "  'Y_validation': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_2_Y_validation.csv',\n",
       "  'X_validation': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_2_X_validation.csv',\n",
       "  'Y_test': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_2_Y_test.csv',\n",
       "  'X_test': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_2_X_test.csv'},\n",
       " {'Y_train': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_3_Y_train.csv',\n",
       "  'X_train': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_3_X_train.csv',\n",
       "  'Y_validation': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_3_Y_validation.csv',\n",
       "  'X_validation': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_3_X_validation.csv',\n",
       "  'Y_test': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_3_Y_test.csv',\n",
       "  'X_test': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_3_X_test.csv'},\n",
       " {'Y_train': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_4_Y_train.csv',\n",
       "  'X_train': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_4_X_train.csv',\n",
       "  'Y_validation': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_4_Y_validation.csv',\n",
       "  'X_validation': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_4_X_validation.csv',\n",
       "  'Y_test': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_4_Y_test.csv',\n",
       "  'X_test': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_4_X_test.csv'},\n",
       " {'Y_train': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_5_Y_train.csv',\n",
       "  'X_train': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_5_X_train.csv',\n",
       "  'Y_validation': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_5_Y_validation.csv',\n",
       "  'X_validation': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_5_X_validation.csv',\n",
       "  'Y_test': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_5_Y_test.csv',\n",
       "  'X_test': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_5_X_test.csv'},\n",
       " {'Y_train': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_6_Y_train.csv',\n",
       "  'X_train': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_6_X_train.csv',\n",
       "  'Y_validation': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_6_Y_validation.csv',\n",
       "  'X_validation': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_6_X_validation.csv',\n",
       "  'Y_test': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_6_Y_test.csv',\n",
       "  'X_test': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_6_X_test.csv'},\n",
       " {'Y_train': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_7_Y_train.csv',\n",
       "  'X_train': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_7_X_train.csv',\n",
       "  'Y_validation': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_7_Y_validation.csv',\n",
       "  'X_validation': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_7_X_validation.csv',\n",
       "  'Y_test': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_7_Y_test.csv',\n",
       "  'X_test': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_7_X_test.csv'},\n",
       " {'Y_train': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_8_Y_train.csv',\n",
       "  'X_train': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_8_X_train.csv',\n",
       "  'Y_validation': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_8_Y_validation.csv',\n",
       "  'X_validation': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_8_X_validation.csv',\n",
       "  'Y_test': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_8_Y_test.csv',\n",
       "  'X_test': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_8_X_test.csv'},\n",
       " {'Y_train': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_9_Y_train.csv',\n",
       "  'X_train': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_9_X_train.csv',\n",
       "  'Y_validation': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_9_Y_validation.csv',\n",
       "  'X_validation': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_9_X_validation.csv',\n",
       "  'Y_test': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_9_Y_test.csv',\n",
       "  'X_test': 's3://sagemaker-eu-west-1-729071960169/numerai\\\\explore/batch_9_X_test.csv'}]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models \n",
    "\n",
    "The third step is setting up the models. We define general parameters as well as each model we want to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "# Set some defaults so we don't need to type this every time\n",
    "training_instance_count = 1\n",
    "training_instance_type = \"ml.m4.xlarge\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"role\": role,\n",
    "    \"train_instance_count\": training_instance_count,\n",
    "    \"train_instance_type\": training_instance_type,\n",
    "    \"sagemaker_session\": session,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_learner = sagemaker.LinearLearner(\n",
    "    predictor_type=\"regressor\", #Also try multiclass?\n",
    "    epochs=10,\n",
    "    optimizer=\"auto\", #Can also choose sgd, adam or rmsprop\n",
    "    loss=\"auto\", #Can also choose ‘logistic’, ‘squared_loss’, ‘absolute_loss’, ‘hinge_loss’, ‘eps_insensitive_squared_loss’, ‘eps_insensitive_absolute_loss’, ‘quantile_loss’, ‘huber_loss’ \n",
    "    learning_rate=None,\n",
    "    output_path=os.path.join(prefix, \"linear_learner\"),\n",
    "    **model_kwargs,\n",
    ")\n",
    "models.append(linear_learner)\n",
    "# For multiclass there is also accuracy_top_k, f_beta and balance_multiclass_weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_learner_hyperparameters = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.estimator import Estimator\n",
    "xgboost_container = get_image_uri(region, 'xgboost')\n",
    "xgboost = Estimator(\n",
    "    xgboost_container,\n",
    "    output_path=os.path.join(prefix, \"xgboost\"),\n",
    "    hyperparameters={\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"silent\":\"0\",\n",
    "        \"objective\":\"reg:linear\",\n",
    "        \"num_round\":\"50\",\n",
    "    }\n",
    "    **model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
